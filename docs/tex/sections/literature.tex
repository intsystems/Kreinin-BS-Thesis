\section{Обзор литературы}

Стохастические методы имеют обширный анализ их сходимости \citep{schneider2007stochastic, heyman2004stochastic, spall1998implementation}, в то время как методы, включающие предобуславливаниям, являются относительно новыми и неизученными. В одной из первых работ по предобуславливанию \citep{duchi2011adaptive} авторы провели тщательный анализ теории сходимости Adagrad.
Однако в более поздних работах, например, обсуждающих RMSProp \citep{rmsprop} или Adam \citep{kingma2014adam}, теоретическим аспектам уделяется мало внимания, либо существующая теория содержит неточности в доказательстве.

Со временем ошибки были исправлены, что привело к разработке надежных теорий сходимости для методов с предобуславливанием \citep{reddi2019convergence, defossez2020simple}.
В другом исследовании Лощилов и Хуттер \citep{loshchilov2017decoupled} исследовали свойства алгоритмов Adam и AdamW в терминах гиперпараметров, а также изучили методы рестартов. Чжан и др.\citep{zhang2019lookahead} исследовали механизм заглядывания в будущее в Adam. В \citep{ginsburg2020stochastic} исследователи из Nvidia предложили новый способ добавления регуляризации в алгоритм Adam, который на их экспериментах дал прирост в качестве обучения.
Совсем недавно были созданы теории сходимости для современных методов, таких как OASIS \citep{goldberg2011oasis, sadiev2022stochastic}.
Кроме того, появилась теория, рассматривающая изменяющиеся во времени матрицы предобуславливания \citep{beznosikov2022scaled}.
Тем не менее, многие вопросы в этой области остаются без ответа.
Некоторые из них сформулированы в конце предыдущего параграфа и рассматриваются в нашей статье.

