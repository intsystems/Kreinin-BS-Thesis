\subsection{Затухание весов}
Как было сказано выше, в методах с предобуславливанием существует несколько техник добавления регуляризации в оптимзируемую функцию. Мы рассмотрим три различных подхода, которые проиллюстрированы в \hyperref[alg:precond]{Алгоритм 1} с помощью различных цветов (каждый отдельный цвет это отдельный алгоритм).

\begin{algorithm}[H]
    \caption{Различные способы использования предобуславливания с регуляризацией}
    \label{alg:precond}
    
    \begin{algorithmic}
            \Require{$\eta$ $-$ шаг обучения, $f$ $-$ оптимзируемая функция}
            
            \While {$w$ не сойдется}
            \State $t = t+1$
            \State $g_t \gets$ стохастический градиент $f$
            \State $\textcolor{blue}{g_t \gets g_t + \nabla r(w_t)}$ \hfill \textcolor{blue}{обычная регуляризация}
            \State $D_t \gets$ матрица предобуславливания с помощью $g_t$

            \State \textcolor{blue}{$w_t \gets w_{t-1} - \eta \cdot D_t^{-1}g_t $} \hfill \textcolor{blue}{обычная регуляризация}, 
            \State \textcolor{orange}{$w_t \gets w_{t-1} - \eta \cdot D_t^{-1} \left(g_t +\nabla r(w_t) \right)$} \hfill \textcolor{orange}{масштабированное затухание весов}, 
            \State \textcolor{red}{$w_t \gets w_{t-1} - \eta \cdot D_t^{-1} g_t  - \eta \cdot \nabla r(w_t)$} \hfill \textcolor{red}{затухание весов}, 
            \EndWhile
    \end{algorithmic}
\end{algorithm}

Если говорить более конкретно, то первая техника регуляризации, показанная в \textcolor{blue}{синим}, заключается в простом добавлении регуляризационного члена к оптизируемой функции. Этот регуляризатор включается в стохастический градиент и учитывается при вычислении $D_t$. По сути, этот подход предполагает применение базового метода оптимизации с предобусловляиванием к задаче \eqref{F_big}.
Вторая техника регуляризации, показанная на рисунке \textcolor{orange}{оранжевым}, является новым подходом. Хотя член регуляризатора не влияет на вычисление матрицы предобуславливания $D_t$, он добавляется перед применением $D_t$. Это означает, что скорость обучения принимается одинаковой для градиента и регуляризатора.
Последний рассматриваемый нами подход к регуляризации известен как затухание веса, в алгоритме он подсвечивается цветом \textcolor{red}{красным} в алгоритме \ref{alg:precond}.
Как и во втором методе, матрица $D_t$ вычисляется без использования регуляризатора, а в этом методе регуляризатор включен на шаге обновления весов, что позволяет изьежать влияния регуляризации на матрицу предобуславливания.

Важно учитывать влияние регуляризации при разработке алгоритмов оптимизации, и я надеюсь, что моё исследование окажется полезным для исследователей в этой области.
