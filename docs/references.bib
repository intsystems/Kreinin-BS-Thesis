@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{jahani2021doubly,
  title={Doubly adaptive scaled algorithm for machine learning using second-order information},
  author={Jahani, Majid and Rusakov, Sergey and Shi, Zheng and Richt{\'a}rik, Peter and Mahoney, Michael W and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:2109.05198},
  year={2021}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{poggio1987computational,
  title={Computational vision and regularization theory},
  author={Poggio, Tomaso and Torre, Vincent and Koch, Christof},
  journal={Readings in computer vision},
  pages={638--643},
  year={1987},
  publisher={Elsevier}
}

@article{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{sadiev2022stochastic,
  title={Stochastic gradient methods with preconditioned updates},
  author={Sadiev, Abdurakhmon and Beznosikov, Aleksandr and Almansoori, Abdulla Jasem and Kamzolov, Dmitry and Tappenden, Rachael and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:2206.00285},
  year={2022}
}
@article{rmsprop,
title={ Lecture 6.5 - rmsprop: Divide the gradient by
a running average of its recent magnitude},
author={T. Tieleman and G. Hinton.},
journal={Lecture 6.5 - rmsprop: Divide the gradient by
a running average of its recent magnitude},
year={2012}
}

@misc{yao2021adahessian,
      title={ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning}, 
      author={Zhewei Yao and Amir Gholami and Sheng Shen and Mustafa Mustafa and Kurt Keutzer and Michael W. Mahoney},
      year={2021},
      eprint={2006.00719},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{beznosikov2022scaled,
  title={On scaled methods for saddle point problems},
  author={Beznosikov, Aleksandr and Alanov, Aibek and Kovalev, Dmitry and Tak{\'a}{\v{c}}, Martin and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2206.08303},
  year={2022}
}

@article{stich2019unified,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{goldberg2011oasis,
  title={Oasis: Online active semi-supervised learning},
  author={Goldberg, Andrew and Zhu, Xiaojin and Furger, Alex and Xu, Jun-Ming},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={25},
  number={1},
  pages={362--367},
  year={2011}
}

@book{schneider2007stochastic,
  title={Stochastic optimization},
  author={Schneider, Johannes and Kirkpatrick, Scott},
  year={2007},
  publisher={Springer Science \& Business Media}
}

@article{spall1998implementation,
  title={Implementation of the simultaneous perturbation algorithm for stochastic optimization},
  author={Spall, James C},
  journal={IEEE Transactions on aerospace and electronic systems},
  volume={34},
  number={3},
  pages={817--823},
  year={1998},
  publisher={IEEE}
}

@book{heyman2004stochastic,
  title={Stochastic models in operations research: stochastic optimization},
  author={Heyman, Daniel P and Sobel, Matthew J},
  volume={2},
  year={2004},
  publisher={Courier Corporation}
}

@article{dennis1977quasi,
  title={Quasi-Newton methods, motivation and theory},
  author={Dennis, Jr, John E and Mor{\'e}, Jorge J},
  journal={SIAM review},
  volume={19},
  number={1},
  pages={46--89},
  year={1977},
  publisher={SIAM}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{cunningham2008supervised,
  title={Supervised learning},
  author={Cunningham, P{\'a}draig and Cord, Matthieu and Delany, Sarah Jane},
  journal={Machine learning techniques for multimedia: case studies on organization and retrieval},
  pages={21--49},
  year={2008},
  publisher={Springer}
}

@article{rifkin2007notes,
  title={Notes on regularized least squares},
  author={Rifkin, Ryan M and Lippert, Ross A},
  year={2007}
}

@inproceedings{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
  pages={177--186},
  year={2010},
  organization={Springer}
}

@article{defossez2020simple,
  title={A simple convergence proof of adam and adagrad},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{li2017cifar10,
  title={Cifar10-dvs: an event-stream dataset for object classification},
  author={Li, Hongmin and Liu, Hanchao and Ji, Xiangyang and Li, Guoqi and Shi, Luping},
  journal={Frontiers in neuroscience},
  volume={11},
  pages={309},
  year={2017},
  publisher={Frontiers Media SA}
}

@techreport{sobieszczanski1982linear,
  title={A linear decomposition method for large optimization problems. Blueprint for development},
  author={Sobieszczanski-Sobieski, Jaroslaw},
  year={1982}
}

@article{zhang2018three,
  title={Three mechanisms of weight decay regularization},
  author={Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
  journal={arXiv preprint arXiv:1810.12281},
  year={2018}
}

@article{ramzan2020deep,
  title={A deep learning approach for automated diagnosis and multi-class classification of Alzheimerâ€™s disease stages using resting-state fMRI and residual neural networks},
  author={Ramzan, Farheen and Khan, Muhammad Usman Ghani and Rehmat, Asim and Iqbal, Sajid and Saba, Tanzila and Rehman, Amjad and Mehmood, Zahid},
  journal={Journal of medical systems},
  volume={44},
  pages={1--16},
  year={2020},
  publisher={Springer}
}

@article{chapelle2000vicinal,
  title={Vicinal risk minimization},
  author={Chapelle, Olivier and Weston, Jason and Bottou, L{\'e}on and Vapnik, Vladimir},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@article{hazan2007adaptive,
  title={Adaptive online gradient descent},
  author={Hazan, Elad and Rakhlin, Alexander and Bartlett, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={20},
  year={2007}
}

@article{zhou2017improved,
  title={Improved regularization techniques for end-to-end speech recognition},
  author={Zhou, Yingbo and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07108},
  year={2017}
}

@inproceedings{wu2022stgn,
  title={STGN: an Implicit Regularization Method for Learning with Noisy Labels in Natural Language Processing},
  author={Wu, Tingting and Ding, Xiao and Tang, Minji and Zhang, Hao and Qin, Bing and Liu, Ting},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={7587--7598},
  year={2022}
}

@inproceedings{zhu2017learning,
  title={Learning spatial regularization with image-level supervisions for multi-label image classification},
  author={Zhu, Feng and Li, Hongsheng and Ouyang, Wanli and Yu, Nenghai and Wang, Xiaogang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5513--5522},
  year={2017}
}

@article{girosi1995regularization,
  title={Regularization theory and neural networks architectures},
  author={Girosi, Federico and Jones, Michael and Poggio, Tomaso},
  journal={Neural computation},
  volume={7},
  number={2},
  pages={219--269},
  year={1995},
  publisher={MIT Press}
}

@misc{reddi2019convergence,
      title={On the Convergence of Adam and Beyond}, 
      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
      year={2019},
      eprint={1904.09237},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{zhang2019lookahead,
      title={Lookahead Optimizer: k steps forward, 1 step back}, 
      author={Michael R. Zhang and James Lucas and Geoffrey Hinton and Jimmy Ba},
      year={2019},
      eprint={1907.08610},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ginsburg2020stochastic,
      title={Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks}, 
      author={Boris Ginsburg and Patrice Castonguay and Oleksii Hrinchuk and Oleksii Kuchaiev and Vitaly Lavrukhin and Ryan Leary and Jason Li and Huyen Nguyen and Yang Zhang and Jonathan M. Cohen},
      year={2020},
      eprint={1905.11286},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}